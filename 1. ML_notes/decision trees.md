###### Tags - [[Machine-Learning]] [[Data-Mining]],
##### Related topics -  [[Tree Ensemble]], [[Entropy]], [[Gini Index]], [[One Hot Encoding]]

#### Notes-
***1.explanation:-***
- **Decision Trees (DTs)** are a non-parametric supervised learning method used for [[Classification]] and [[Regression]]. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.
	-------------------------------------------------------------------------------------------------      
	
- In simple words - **Decision Trees (DTs)** are a simple yet powerful tool used in machine learning to make decisions. Imagine you have a set of questions to ask in order to make a choice. Each question is based on certain features (or characteristics) of the data, and depending on the answer, you move down a path until you reach a conclusion.
- For example, if you’re trying to decide whether to bring an umbrella, the tree might ask:
	1. Is it cloudy? 
		  - If yes → Check if there’s a chance of rain.
		  - If no → Don’t bring an umbrella.
	------------------------------------------------------------------------------------------------
	In a Decision Tree, each "question" is a decision rule, and the final "conclusion" is a prediction. The goal is to split the data into groups that are as similar as possible at the end. It's like breaking a big problem down into smaller, easy-to-handle steps. 
	DTs are used for both **classification** (when you're putting something into a category, like "yes" or "no") and **regression** (when you're predicting a number, like the price of a house). It’s a visual, intuitive way to understand how decisions are made based on data.
	----------------------------------------------------------------------------------------------------
	The top most element is called the root node and all subsequent nodes are called as decision nodes
![[Elements of decision-tress.png|700]]

***2.Measuring the purity of a set of example***
- **[[Entropy]]** is used as a measure of purity. So it basically measure how well distributed the different classes in our data set are. For example in a dataset containing 3 cats and 3 dogs the entropy would be 1. Similarly if it contained 6 cats and no dogs then the entropy would be 0.
- let p1 be the fraction of examples that are cats. The entropy function $H(p_1)$ would be defined as:-	$$\begin{align} p_0 &=1-p_1 \\ 
		                   H(p_1) &= -p_1log_2(p_1) - p_0log_2(p_0) \\
		                   &= -p_1log_2(p_1) - (1-p_1)log_2(1-p_1) \end{align}$$
		\*\*note If $p_0=0$ then we assume that $0log(0)=0$
		----------------------------------------------------------------------------------------------
- In generalized form for N number of different classes  the formula for Entropy is as follows. 
		![[Entropy formula.png]]
		note that $p_i$ in our case would be fraction of examples that are cats.
- ***This function looks like the loss function for logistic regression. why??***
		-----------------------------------------------------------------------------------------	
		the curve generated by the formula looks like this
		![[Entropy curve.png|500]]

***3.Choosing a split*** 
- We need to consider the weighted average of both the entropy and the size of the set. For example if we consider an set with high entropy but less size it is not as bad as a set with a lot of examples and having a high entropy, the second one is less impure
 - attributed with each node is entropy of its sub branches we calculate a weighted average of entropy of both sub-branches and the no. of examples that went into that node.
 - But for choosing the split we actually check the difference between entropy after we split and entropy at the root node.
 - If the reduction in entropy is low then we can stop splitting otherwise we risk over-fitting
 - Therefore Information gain can be calculated as
		 ![[Information gain formula.png]]

***4.Logic of a decision tree*** 
 - Start with all the examples at the root node
 - Calculate information gain for all possible features , and pick the one with the highest information gain
 - Split dataset according to selected feature, and create the left and right branches of the tree
 - keep repeating splitting process until stopping criteria is meet
	1. When a node is 100% one class
	2. When splitting a node will result in the tree exceeding a maximum depth
	3. Information gain from additional splits is less than a threshold
		 

***5.Additional topics to consider*** 
 - [One hot encoding] 
 - [splitting on a continuous variable]
 - [regression with decision trees]
#### Research Papers and Articles -
[Song YY, Lu Y. Decision tree methods: applications for classification and prediction. Shanghai Arch Psychiatry. 2015 Apr 25;27(2):130-5. doi: 10.11919/j.issn.1002-0829.215044. PMID: 26120265; PMCID: PMC4466856.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4466856/)

[Entropy Calculation, Information Gain & Decision Tree Learning](https://medium.com/analytics-vidhya/entropy-calculation-information-gain-decision-tree-learning-771325d16f)

[ An insight to Decision Trees](https://tutswiki.com/machine-learning/decision-trees/)


##### Books -
Classification and Regression Trees By [Leo Breiman], [Jerome Friedman], [R.A. Olshen], [Charles J. Stone]
#### YT links - 

#### Potential Project Ideas - 

