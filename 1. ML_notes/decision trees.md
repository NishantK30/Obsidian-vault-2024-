###### Tags - [[Machine-Learning]] [[Data-Mining]], [[Deep-Learning]]
##### Related topics -  [[tree ensemble]], [[entropy]],[[gini index]]

#### Notes-
***1.explanation:-***
	**Decision Trees (DTs)** are a non-parametric supervised learning method used for [[classification]] and [[regression]]. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.
	--------------------------------------------------------------------------------------------------
	In simple words - **Decision Trees (DTs)** are a simple yet powerful tool used in machine learning to make decisions. Imagine you have a set of questions to ask in order to make a choice. Each question is based on certain features (or characteristics) of the data, and depending on the answer, you move down a path until you reach a conclusion.
	For example, if you’re trying to decide whether to bring an umbrella, the tree might ask: 
	1. Is it cloudy? 
	   - If yes → Check if there’s a chance of rain.
	   - If no → Don’t bring an umbrella.
	------------------------------------------------------------------------------------------------
	In a Decision Tree, each "question" is a decision rule, and the final "conclusion" is a prediction. The goal is to split the data into groups that are as similar as possible at the end. It's like breaking a big problem down into smaller, easy-to-handle steps. 
	DTs are used for both **classification** (when you're putting something into a category, like "yes" or "no") and **regression** (when you're predicting a number, like the price of a house). It’s a visual, intuitive way to understand how decisions are made based on data.
	----------------------------------------------------------------------------------------------------
	The top most element is called the root node and all subsequent nodes are called as decision nodes
![[Elements of decision-tress.png|700]]

***2.Measuring the purity of a set of example***
		**[[entropy]]** is used as a measure of purity. So it basically measure how well distributed the different classes in our data set are. For example in a dataset containing 3 cats and 3 dogs the entropy would be 1. Similarly if it contained 6 cats and no dogs then the entropy would be 0.
		 let p1 be the fraction of examples that are cats. The entropy function $H(p_1)$ would be defined as:-	$$\begin{align} p_0 &=1-p_1 \\ 
		                   H(p_1) &= -p_1log_2(p_1) - p_0log_2(p_0) \\
		                   &= -p_1log_2(p_1) - (1-p_1)log_2(1-p_1) \end{align}$$
		\*\*note If $p_0=0$ then we assume that $0log(0)=0$
		----------------------------------------------------------------------------------------------
		 In generalized form for N number of different classes  the formula for Entropy is as follows. 
		![[Entropy formula.png]]
		note that $p_i$ in our case would be fraction of examples that are cats.
		***This function looks like the loss function for logistic regression. why??***
		-----------------------------------------------------------------------------------------			the curve generated by the formula looks like this
		![[Entropy curve.png|500]]

#### Research Papers and Articles -
[Song YY, Lu Y. Decision tree methods: applications for classification and prediction. Shanghai Arch Psychiatry. 2015 Apr 25;27(2):130-5. doi: 10.11919/j.issn.1002-0829.215044. PMID: 26120265; PMCID: PMC4466856.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4466856/)


##### Books -
Classification and Regression Trees By [Leo Breiman], [Jerome Friedman], [R.A. Olshen], [Charles J. Stone]
#### YT links - 

#### Potential Project Ideas - 

