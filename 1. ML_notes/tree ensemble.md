###### Tags- [[Machine-Learning]]
##### Related topics:-[[random forest algorithm]],[[XGBoost]],[[Sampling with replacement]]

### Notes - 
- One of the weaknesses of using a single decision tree is that decision tree can be highly sensitive to small changes in the data. 
- One solution to make the algorithm less sensitive or more robust is to build not one decision tree, but to build a lot of decision trees, and we call that a tree ensemble.
###### Sampling with replacement
- In this method we will create multiple random training sets that are all slightly different from our original training set.
- We will use this create new training sets
###### Random Forest algorithm
- At each node, when choosing a features to use to split, if n features are available , pick a random subset of k< n features and allow the algorithm to only choose from that subset of features.
###### XGBoost (eXtreme Gradient Boosting)
- In this Algorithm, we do a process similar to random forest algorithm. But each subsequent tree after the first will try to pick the training set such that it contains the examples that the previous tree failed to predict or classify correctly.
#### Research Papers-
- [The Comparison of Tree-Based Ensemble Machine Learning for Classifying Public Datasets](https://proceeding.researchsynergypress.com/index.php/cset/article/view/412) [pdf](obsidian://open?vault=Nishant2024&file=Attachments%2FThe%20Comparison%20of%20Tree-Based%20Ensemble%20Machine%20Learning%20for%20Classifying%20Public%20Datasets.pdf)
- [An Empirical Comparison of Tree-Based Learning Algorithms: An Egyptian Rice Diseases Classification Case Study](https://www.researchgate.net/publication/290212859_An_Empirical_Comparison_of_Tree-Based_Learning_Algorithms_An_Egyptian_Rice_Diseases_Classification_Case_Study)  [pdf](obsidian://open?vault=Nishant2024&file=Attachments%2FAn_Empirical_Comparison_of_Tree-Based_Learning_Alg.pdf)





#### YT links - 



#### Potential Project Ideas
